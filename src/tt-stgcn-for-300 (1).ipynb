{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH DEFINITION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Graph:\n",
    "    def __init__(self, layout='mediapipe_65', strategy='uniform'):\n",
    "        self.num_node = 65\n",
    "        self.self_link = [(i, i) for i in range(self.num_node)]\n",
    "        \n",
    "        self.inward_bone_link = [\n",
    "            (12, 14), (14, 16), (16, 18), (16, 20), (16, 22), (18, 20),\n",
    "            (11, 13), (13, 15), (15, 17), (15, 19), (15, 21), (17, 19),\n",
    "            (12, 11),\n",
    "            (10, 8), (8, 6), (6, 5), (5, 4), (4, 0),\n",
    "            (9, 7), (7, 3), (3, 2), (2, 1), (1, 0),\n",
    "            (15, 23), (16, 44)\n",
    "        ]\n",
    "        \n",
    "        for hand_start_idx in [23, 44]:\n",
    "            self.inward_bone_link.extend([\n",
    "                (hand_start_idx, hand_start_idx + 1), \n",
    "                (hand_start_idx + 1, hand_start_idx + 2),\n",
    "                (hand_start_idx + 2, hand_start_idx + 3), \n",
    "                (hand_start_idx + 3, hand_start_idx + 4)\n",
    "            ])\n",
    "            for finger_start in range(5, 21, 4):\n",
    "                self.inward_bone_link.extend([\n",
    "                    (hand_start_idx, hand_start_idx + finger_start),\n",
    "                    (hand_start_idx + finger_start, hand_start_idx + finger_start + 1),\n",
    "                    (hand_start_idx + finger_start + 1, hand_start_idx + finger_start + 2),\n",
    "                    (hand_start_idx + finger_start + 2, hand_start_idx + finger_start + 3)\n",
    "                ])\n",
    "\n",
    "        self.edge = self.self_link + self.inward_bone_link\n",
    "        self.A = torch.zeros(self.num_node, self.num_node)\n",
    "        for i, j in self.edge:\n",
    "            self.A[j, i] = 1\n",
    "            self.A[i, j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Archectecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T07:45:23.284811Z",
     "iopub.status.busy": "2025-11-02T07:45:23.284618Z",
     "iopub.status.idle": "2025-11-02T07:45:27.614534Z",
     "shell.execute_reply": "2025-11-02T07:45:27.613703Z",
     "shell.execute_reply.started": "2025-11-02T07:45:23.284794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,136,005\n",
      "Output shape: torch.Size([2, 300])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "#2  EFFICIENT DEPTHWISE-SEPARABLE CONVOLUTIONS\n",
    "# ============================================================================\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"Efficient convolution: 8-10x fewer parameters than standard conv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=1, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_ch, in_ch, kernel_size, stride, padding, groups=in_ch)\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. LIGHTWEIGHT MULTI-HEAD SELF-ATTENTION\n",
    "# ============================================================================\n",
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Reduced-dimension attention for temporal modeling\"\"\"\n",
    "    def __init__(self, d_model, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Use 1x1 convs instead of linear for efficiency\n",
    "        self.qkv = nn.Conv1d(d_model, d_model * 3, 1)\n",
    "        self.proj = nn.Conv1d(d_model, d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, V) -> reshape to (B*V, C, T)\n",
    "        B, C, T, V = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).reshape(B * V, C, T)\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B * V, 3, self.num_heads, self.head_dim, T)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn = torch.einsum('bhdt,bhds->bhts', q, k) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.einsum('bhts,bhds->bhdt', attn, v)\n",
    "        out = out.reshape(B * V, C, T)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        # Reshape back\n",
    "        out = out.reshape(B, V, C, T).permute(0, 2, 3, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TEMPORAL TRANSFORMER BLOCK\n",
    "# ============================================================================\n",
    "class TemporalTransformerBlock(nn.Module):\n",
    "    \"\"\"Lightweight transformer for capturing long-range temporal dependencies\"\"\"\n",
    "    def __init__(self, channels, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = EfficientMultiHeadAttention(channels, num_heads, dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            DepthwiseSeparableConv(channels, channels * 2, kernel_size=1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(channels * 2, channels, 1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, V)\n",
    "        B, C, T, V = x.shape\n",
    "        \n",
    "        # Attention with residual\n",
    "        x_norm = x.permute(0, 2, 3, 1)  # (B, T, V, C)\n",
    "        x_norm = self.norm1(x_norm).permute(0, 3, 1, 2)  # Back to (B, C, T, V)\n",
    "        attn_out = self.attn(x_norm)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # FFN with residual\n",
    "        x_norm = x.permute(0, 2, 3, 1)\n",
    "        x_norm = self.norm2(x_norm).permute(0, 3, 1, 2)\n",
    "        ffn_out = self.ffn(x_norm)\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ADAPTIVE GRAPH CONVOLUTION (learnable adjacency)\n",
    "# ============================================================================\n",
    "class AdaptiveGCN(nn.Module):\n",
    "    \"\"\"Graph convolution with learnable adjacency matrix\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, A, num_subsets=3):\n",
    "        super().__init__()\n",
    "        self.num_subsets = num_subsets\n",
    "        \n",
    "        # Learnable adjacency matrices\n",
    "        self.A = nn.Parameter(torch.stack([A] * num_subsets), requires_grad=True)\n",
    "        \n",
    "        # Efficient depthwise-separable convs\n",
    "        self.convs = nn.ModuleList([\n",
    "            DepthwiseSeparableConv(in_ch, out_ch, kernel_size=1) \n",
    "            for _ in range(num_subsets)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, V)\n",
    "        N, C, T, V = x.shape\n",
    "        out = None\n",
    "        \n",
    "        for i in range(self.num_subsets):\n",
    "            # Graph convolution: (B, C, T, V) @ (V, V) -> (B, C, T, V)\n",
    "            x_reshaped = x.view(N, C * T, V)\n",
    "            z = torch.matmul(x_reshaped, self.A[i]).view(N, C, T, V)\n",
    "            z = self.convs[i](z)\n",
    "            out = z if out is None else out + z\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ENHANCED ST-GCN BLOCK\n",
    "# ============================================================================\n",
    "class EnhancedSTGCNBlock(nn.Module):\n",
    "    \"\"\"ST-GCN + Temporal Transformer + Cross-Stream Attention\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, A, stride=1, use_transformer=True):\n",
    "        super().__init__()\n",
    "        self.gcn = AdaptiveGCN(in_ch, out_ch, A)\n",
    "        self.tcn = DepthwiseSeparableConv(\n",
    "            out_ch, out_ch, kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)\n",
    "        )\n",
    "        \n",
    "        self.use_transformer = use_transformer\n",
    "        if use_transformer:\n",
    "            self.transformer = TemporalTransformerBlock(out_ch, num_heads=4)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 1, stride=(stride, 1)),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        ) if in_ch != out_ch or stride != 1 else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.residual(x)\n",
    "        x = self.gcn(x)\n",
    "        x = self.tcn(x)\n",
    "        \n",
    "        if self.use_transformer:\n",
    "            x = self.transformer(x)\n",
    "        \n",
    "        return F.silu(x + res)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. CROSS-STREAM FUSION MODULE\n",
    "# ============================================================================\n",
    "class CrossStreamFusion(nn.Module):\n",
    "    \"\"\"Cross-attention between joint and bone streams\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 4, 1)\n",
    "        self.key = nn.Conv2d(channels, channels // 4, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # x1: joint stream, x2: bone stream\n",
    "        B, C, T, V = x1.shape\n",
    "        \n",
    "        # Compute attention: x1 queries x2\n",
    "        q = self.query(x1).view(B, -1, T * V)  # (B, C/4, T*V)\n",
    "        k = self.key(x2).view(B, -1, T * V)    # (B, C/4, T*V)\n",
    "        v = self.value(x2).view(B, C, T * V)   # (B, C, T*V)\n",
    "        \n",
    "        attn = torch.bmm(q.transpose(1, 2), k)  # (B, T*V, T*V)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = torch.bmm(v, attn.transpose(1, 2))  # (B, C, T*V)\n",
    "        out = out.view(B, C, T, V)\n",
    "        \n",
    "        return x1 + self.gamma * out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. MAIN MODEL: TT-STGCN\n",
    "# ============================================================================\n",
    "class TT_STGCN(nn.Module):\n",
    "    \"\"\"Temporal Transformer ST-GCN with Cross-Stream Fusion\"\"\"\n",
    "    def __init__(self, num_classes, num_joints=65, \n",
    "                 joint_in_ch=4, bone_in_ch=2, base_ch=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        graph = Graph()\n",
    "        A = graph.A\n",
    "        \n",
    "        # Input normalization\n",
    "        self.bn_joint = nn.BatchNorm2d(joint_in_ch)\n",
    "        self.bn_bone = nn.BatchNorm2d(bone_in_ch)\n",
    "        \n",
    "        # ---- STREAM 1: Joint Stream ----\n",
    "        self.joint_stream = nn.Sequential(\n",
    "            EnhancedSTGCNBlock(joint_in_ch, base_ch, A, use_transformer=False),\n",
    "            EnhancedSTGCNBlock(base_ch, base_ch * 2, A, use_transformer=True),\n",
    "            EnhancedSTGCNBlock(base_ch * 2, base_ch * 3, A, stride=2, use_transformer=True)\n",
    "        )\n",
    "        \n",
    "        # ---- STREAM 2: Bone Stream ----\n",
    "        self.bone_stream = nn.Sequential(\n",
    "            EnhancedSTGCNBlock(bone_in_ch, base_ch, A, use_transformer=False),\n",
    "            EnhancedSTGCNBlock(base_ch, base_ch * 2, A, use_transformer=True),\n",
    "            EnhancedSTGCNBlock(base_ch * 2, base_ch * 3, A, stride=2, use_transformer=True)\n",
    "        )\n",
    "        \n",
    "        # Cross-stream fusion\n",
    "        self.fusion = CrossStreamFusion(base_ch * 3)\n",
    "        \n",
    "        # Post-fusion layers\n",
    "        self.post_fusion = nn.Sequential(\n",
    "            EnhancedSTGCNBlock(base_ch * 3, base_ch * 4, A, stride=2, use_transformer=True),\n",
    "            EnhancedSTGCNBlock(base_ch * 4, base_ch * 6, A, use_transformer=True)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(base_ch * 6, num_classes)\n",
    "        \n",
    "    def forward(self, x_joint, x_bone):\n",
    "        # Normalize\n",
    "        x_joint = self.bn_joint(x_joint)\n",
    "        x_bone = self.bn_bone(x_bone)\n",
    "        \n",
    "        # Process streams\n",
    "        f_joint = self.joint_stream(x_joint)\n",
    "        f_bone = self.bone_stream(x_bone)\n",
    "        \n",
    "        # Cross-stream fusion\n",
    "        f_fused = self.fusion(f_joint, f_bone)\n",
    "        \n",
    "        # Post-fusion\n",
    "        out = self.post_fusion(f_fused)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.pool(out).view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. MODEL SUMMARY\n",
    "# ============================================================================\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = TT_STGCN(num_classes=300, base_ch=32)\n",
    "    print(f\"Total Parameters: {count_parameters(model):,}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    x_joint = torch.randn(2, 4, 64, 65)\n",
    "    x_bone = torch.randn(2, 2, 64, 65)\n",
    "    out = model(x_joint, x_bone)\n",
    "    print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T07:46:10.760280Z",
     "iopub.status.busy": "2025-11-02T07:46:10.759997Z",
     "iopub.status.idle": "2025-11-02T07:46:10.800355Z",
     "shell.execute_reply": "2025-11-02T07:46:10.799656Z",
     "shell.execute_reply.started": "2025-11-02T07:46:10.760258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Setup augmentation\\naugmentation = SignLanguageAugmentation(\\n    shear_range=0.1,\\n    rotate_range=20,\\n    scale_range=(0.9, 1.1),\\n    temporal_crop_ratio=0.9,\\n    spatial_drop_prob=0.1,\\n    temporal_mask_prob=0.15\\n)\\n\\n# Create enhanced dataset\\ntrain_dataset = EnhancedSignLanguageDataset(\\n    train_ids, labels_dict, features_dir, \\n    seq_len=64, augmentation=augmentation, training=True\\n)\\n\\n# Use label smoothing\\ncriterion = LabelSmoothingCrossEntropy(smoothing=0.1)\\n\\n# Use AdamW optimizer with weight decay\\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)\\n\\n# Use cosine annealing with warmup\\nscheduler = CosineAnnealingWarmupRestarts(optimizer, warmup_epochs=10, max_epochs=350)\\n\\n# Training loop\\nfor epoch in range(350):\\n    train_loss, train_acc = train_one_epoch_advanced(\\n        model, train_loader, optimizer, criterion, device,\\n        use_mixup=True, mixup_alpha=0.2\\n    )\\n    \\n    val_loss, val_top1, val_top5 = validate_with_tta(\\n        model, val_loader, criterion, device, num_tta=5\\n    )\\n    \\n    scheduler.step()\\n    \\n    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Top-1: {val_top1:.2f}%\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1. ADVANCED DATA AUGMENTATION\n",
    "# ============================================================================\n",
    "class SignLanguageAugmentation:\n",
    "    \"\"\"\n",
    "    Augmentation techniques for skeleton-based sign language recognition\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 shear_range=0.1,\n",
    "                 rotate_range=20,\n",
    "                 scale_range=(0.9, 1.1),\n",
    "                 temporal_crop_ratio=0.9,\n",
    "                 spatial_drop_prob=0.1,\n",
    "                 temporal_mask_prob=0.15):\n",
    "        self.shear_range = shear_range\n",
    "        self.rotate_range = rotate_range\n",
    "        self.scale_range = scale_range\n",
    "        self.temporal_crop_ratio = temporal_crop_ratio\n",
    "        self.spatial_drop_prob = spatial_drop_prob\n",
    "        self.temporal_mask_prob = temporal_mask_prob\n",
    "    \n",
    "    def random_rotate(self, joints):\n",
    "        \"\"\"Random rotation around z-axis\"\"\"\n",
    "        angle = np.random.uniform(-self.rotate_range, self.rotate_range)\n",
    "        angle_rad = np.deg2rad(angle)\n",
    "        cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n",
    "        \n",
    "        rot_matrix = np.array([[cos_a, -sin_a], [sin_a, cos_a]])\n",
    "        \n",
    "        # Apply to x, y coordinates\n",
    "        T, V, C = joints.shape\n",
    "        joints_rot = joints.copy()\n",
    "        joints_rot[:, :, :2] = np.dot(joints[:, :, :2].reshape(-1, 2), rot_matrix.T).reshape(T, V, 2)\n",
    "        \n",
    "        return joints_rot\n",
    "    \n",
    "    def random_scale(self, joints):\n",
    "        \"\"\"Random scaling\"\"\"\n",
    "        scale = np.random.uniform(*self.scale_range)\n",
    "        return joints * scale\n",
    "    \n",
    "    def random_shear(self, joints):\n",
    "        \"\"\"Random shearing transformation\"\"\"\n",
    "        shear_x = np.random.uniform(-self.shear_range, self.shear_range)\n",
    "        shear_y = np.random.uniform(-self.shear_range, self.shear_range)\n",
    "        \n",
    "        shear_matrix = np.array([[1, shear_x], [shear_y, 1]])\n",
    "        \n",
    "        T, V, C = joints.shape\n",
    "        joints_shear = joints.copy()\n",
    "        joints_shear[:, :, :2] = np.dot(joints[:, :, :2].reshape(-1, 2), shear_matrix.T).reshape(T, V, 2)\n",
    "        \n",
    "        return joints_shear\n",
    "    \n",
    "    def temporal_crop(self, joints, bones):\n",
    "        \"\"\"Randomly crop temporal sequence\"\"\"\n",
    "        T = joints.shape[0]\n",
    "        new_T = int(T * self.temporal_crop_ratio)\n",
    "        \n",
    "        if new_T >= T:\n",
    "            return joints, bones\n",
    "        \n",
    "        start = np.random.randint(0, T - new_T + 1)\n",
    "        return joints[start:start+new_T], bones[start:start+new_T]\n",
    "    \n",
    "    def spatial_dropout(self, joints):\n",
    "        \"\"\"Randomly drop spatial joints\"\"\"\n",
    "        T, V, C = joints.shape\n",
    "        mask = np.random.rand(V) > self.spatial_drop_prob\n",
    "        joints_dropped = joints.copy()\n",
    "        joints_dropped[:, ~mask, :] = 0\n",
    "        return joints_dropped\n",
    "    \n",
    "    def temporal_masking(self, joints):\n",
    "        \"\"\"Mask random temporal segments\"\"\"\n",
    "        T = joints.shape[0]\n",
    "        num_masks = int(T * self.temporal_mask_prob)\n",
    "        \n",
    "        if num_masks == 0:\n",
    "            return joints\n",
    "        \n",
    "        joints_masked = joints.copy()\n",
    "        for _ in range(num_masks):\n",
    "            start = np.random.randint(0, T)\n",
    "            joints_masked[start] = 0\n",
    "        \n",
    "        return joints_masked\n",
    "    \n",
    "    def __call__(self, joints, bones, training=True):\n",
    "        \"\"\"Apply augmentation pipeline\"\"\"\n",
    "        if not training:\n",
    "            return joints, bones\n",
    "        \n",
    "        # Geometric augmentations (apply to joints, will affect bones implicitly)\n",
    "        if np.random.rand() > 0.5:\n",
    "            joints = self.random_rotate(joints)\n",
    "        if np.random.rand() > 0.5:\n",
    "            joints = self.random_scale(joints)\n",
    "        if np.random.rand() > 0.5:\n",
    "            joints = self.random_shear(joints)\n",
    "        \n",
    "        # Temporal augmentation\n",
    "        if np.random.rand() > 0.5:\n",
    "            joints, bones = self.temporal_crop(joints, bones)\n",
    "        \n",
    "        # Dropout augmentations\n",
    "        if np.random.rand() > 0.5:\n",
    "            joints = self.spatial_dropout(joints)\n",
    "        if np.random.rand() > 0.5:\n",
    "            joints = self.temporal_masking(joints)\n",
    "        \n",
    "        return joints, bones\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ENHANCED DATASET WITH AUGMENTATION\n",
    "# ============================================================================\n",
    "class EnhancedSignLanguageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_ids, labels_dict, features_dir, seq_len, \n",
    "                 augmentation=None, training=True):\n",
    "        self.video_ids = video_ids\n",
    "        self.labels = labels_dict\n",
    "        self.features_dir = features_dir\n",
    "        self.seq_len = seq_len\n",
    "        self.augmentation = augmentation\n",
    "        self.training = training\n",
    "        \n",
    "        self.bone_to_joint_map = {\n",
    "            0: 13, 1: 15, 2: 14, 3: 16, 4: 11\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.video_ids[idx]\n",
    "        \n",
    "        # Load data\n",
    "        import os\n",
    "        file_path = os.path.join(self.features_dir, f\"{video_id}.npz\")\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        joint_seq = data['joint_sequence'].astype(np.float32)\n",
    "        bone_seq_flat = data['bone_sequence'].astype(np.float32)\n",
    "        \n",
    "        # Create V-dimensional bone tensor\n",
    "        num_frames = bone_seq_flat.shape[0]\n",
    "        bone_seq = np.zeros((num_frames, 65, 2), dtype=np.float32)\n",
    "        for bone_idx, joint_idx in self.bone_to_joint_map.items():\n",
    "            start_col = bone_idx * 2\n",
    "            bone_seq[:, joint_idx, :] = bone_seq_flat[:, start_col:start_col+2]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.augmentation:\n",
    "            joint_seq, bone_seq = self.augmentation(joint_seq, bone_seq, self.training)\n",
    "        \n",
    "        # Handle padding/truncating\n",
    "        original_len = joint_seq.shape[0]\n",
    "        if original_len < self.seq_len:\n",
    "            pad_len = self.seq_len - original_len\n",
    "            joint_seq = np.pad(joint_seq, ((0, pad_len), (0, 0), (0, 0)), 'constant')\n",
    "            bone_seq = np.pad(bone_seq, ((0, pad_len), (0, 0), (0, 0)), 'constant')\n",
    "        elif original_len > self.seq_len:\n",
    "            # Random crop during training, center crop during validation\n",
    "            if self.training:\n",
    "                start = np.random.randint(0, original_len - self.seq_len + 1)\n",
    "            else:\n",
    "                start = (original_len - self.seq_len) // 2\n",
    "            joint_seq = joint_seq[start:start+self.seq_len]\n",
    "            bone_seq = bone_seq[start:start+self.seq_len]\n",
    "        \n",
    "        label = self.labels[video_id]\n",
    "        \n",
    "        # Convert to tensor and permute to (C, T, V)\n",
    "        joint_seq_tensor = torch.tensor(joint_seq).permute(2, 0, 1)\n",
    "        bone_seq_tensor = torch.tensor(bone_seq).permute(2, 0, 1)\n",
    "        \n",
    "        return joint_seq_tensor, bone_seq_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. TRAINING LOOP WITH ALL IMPROVEMENTS\n",
    "# ============================================================================\n",
    "def train_one_epoch_advanced(model, loader, optimizer, criterion, device, \n",
    "                            use_mixup=True, mixup_alpha=0.2):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for joint_batch, bone_batch, labels in loader:\n",
    "        joint_batch = joint_batch.to(device)\n",
    "        bone_batch = bone_batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Apply mixup\n",
    "        if use_mixup and np.random.rand() > 0.5:\n",
    "            joint_batch, bone_batch, labels_a, labels_b, lam = mixup_data(\n",
    "                joint_batch, bone_batch, labels, mixup_alpha\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(joint_batch, bone_batch)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(joint_batch, bone_batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. VALIDATION WITH TTA (TEST TIME AUGMENTATION)\n",
    "# ============================================================================\n",
    "def validate_with_tta(model, loader, criterion, device, num_tta=5):\n",
    "    \"\"\"Test-time augmentation for better accuracy\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for joint_batch, bone_batch, labels in loader:\n",
    "            joint_batch = joint_batch.to(device)\n",
    "            bone_batch = bone_batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Accumulate predictions from multiple augmentations\n",
    "            tta_preds = []\n",
    "            for _ in range(num_tta):\n",
    "                outputs = model(joint_batch, bone_batch)\n",
    "                tta_preds.append(F.softmax(outputs, dim=1))\n",
    "            \n",
    "            # Average predictions\n",
    "            outputs_avg = torch.stack(tta_preds).mean(0)\n",
    "            loss = criterion(outputs_avg.log(), labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_preds.append(outputs_avg.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, 0)\n",
    "    all_labels = torch.cat(all_labels, 0)\n",
    "    \n",
    "    # Top-1 accuracy\n",
    "    _, predicted = all_preds.max(1)\n",
    "    correct = predicted.eq(all_labels).sum().item()\n",
    "    top1_acc = 100. * correct / len(all_labels)\n",
    "    \n",
    "    # Top-5 accuracy\n",
    "    _, pred_top5 = all_preds.topk(5, 1, True, True)\n",
    "    correct_5 = pred_top5.eq(all_labels.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "    top5_acc = 100. * correct_5 / len(all_labels)\n",
    "    \n",
    "    return running_loss / len(loader), top1_acc, top5_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T17:43:53.465566Z",
     "iopub.status.busy": "2025-11-01T17:43:53.465296Z",
     "iopub.status.idle": "2025-11-01T21:15:07.366486Z",
     "shell.execute_reply": "2025-11-01T21:15:07.365330Z",
     "shell.execute_reply.started": "2025-11-01T17:43:53.465545Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration loaded. Batch size: 24, Epochs: 150\n",
      "\n",
      "Loading JSON data...\n",
      "Number of classes: 300\n",
      "Train: 3548, Val: 900, Test: 668\n",
      "\n",
      "Creating datasets with augmentation...\n",
      "Dataloaders created. Train batches: 148\n",
      "\n",
      "Initializing TT-STGCN model...\n",
      "Total trainable parameters: 1,136,005\n",
      "\n",
      "Setting up training components...\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "Epoch 001/150 | Time: 73.2s | LR: 0.000200 | Train Loss: 5.7460 | Train Acc: 0.39% | Val Loss: 5.6403 | Val Top-1: 1.00% | Val Top-5: 4.67%\n",
      "✓ New best model saved! Val Top-1: 1.00%\n",
      "Epoch 002/150 | Time: 80.8s | LR: 0.000300 | Train Loss: 5.5993 | Train Acc: 0.87% | Val Loss: 5.3668 | Val Top-1: 2.22% | Val Top-5: 8.33%\n",
      "✓ New best model saved! Val Top-1: 2.22%\n",
      "Epoch 003/150 | Time: 83.5s | LR: 0.000400 | Train Loss: 5.3640 | Train Acc: 1.35% | Val Loss: 5.0567 | Val Top-1: 3.78% | Val Top-5: 13.22%\n",
      "✓ New best model saved! Val Top-1: 3.78%\n",
      "Epoch 004/150 | Time: 83.7s | LR: 0.000500 | Train Loss: 5.1698 | Train Acc: 2.23% | Val Loss: 4.9018 | Val Top-1: 3.78% | Val Top-5: 14.89%\n",
      "Epoch 005/150 | Time: 83.2s | LR: 0.000600 | Train Loss: 5.0019 | Train Acc: 3.38% | Val Loss: 4.7152 | Val Top-1: 5.00% | Val Top-5: 20.44%\n",
      "✓ New best model saved! Val Top-1: 5.00%\n",
      "Epoch 006/150 | Time: 83.8s | LR: 0.000700 | Train Loss: 4.8639 | Train Acc: 3.61% | Val Loss: 4.6285 | Val Top-1: 7.56% | Val Top-5: 22.22%\n",
      "✓ New best model saved! Val Top-1: 7.56%\n",
      "Epoch 007/150 | Time: 83.6s | LR: 0.000800 | Train Loss: 4.7375 | Train Acc: 4.26% | Val Loss: 4.5169 | Val Top-1: 8.67% | Val Top-5: 27.00%\n",
      "✓ New best model saved! Val Top-1: 8.67%\n",
      "Epoch 008/150 | Time: 82.8s | LR: 0.000900 | Train Loss: 4.6057 | Train Acc: 6.17% | Val Loss: 4.4475 | Val Top-1: 8.56% | Val Top-5: 28.22%\n",
      "Epoch 009/150 | Time: 82.2s | LR: 0.001000 | Train Loss: 4.4903 | Train Acc: 7.95% | Val Loss: 4.2850 | Val Top-1: 12.56% | Val Top-5: 32.78%\n",
      "✓ New best model saved! Val Top-1: 12.56%\n",
      "Epoch 010/150 | Time: 106.0s | LR: 0.001000 | Train Loss: 4.3937 | Train Acc: 8.60% | Val Loss: 4.2205 | Val Top-1: 12.44% | Val Top-5: 34.56%\n",
      "Epoch 011/150 | Time: 82.1s | LR: 0.001000 | Train Loss: 4.2372 | Train Acc: 12.37% | Val Loss: 4.0463 | Val Top-1: 14.22% | Val Top-5: 40.78%\n",
      "✓ New best model saved! Val Top-1: 14.22%\n",
      "Epoch 012/150 | Time: 82.2s | LR: 0.000999 | Train Loss: 4.1128 | Train Acc: 12.18% | Val Loss: 3.9304 | Val Top-1: 18.00% | Val Top-5: 45.56%\n",
      "✓ New best model saved! Val Top-1: 18.00%\n",
      "Epoch 013/150 | Time: 82.1s | LR: 0.000999 | Train Loss: 3.8611 | Train Acc: 15.84% | Val Loss: 3.7865 | Val Top-1: 20.78% | Val Top-5: 51.44%\n",
      "✓ New best model saved! Val Top-1: 20.78%\n",
      "Epoch 014/150 | Time: 81.6s | LR: 0.000998 | Train Loss: 3.7810 | Train Acc: 18.09% | Val Loss: 3.6550 | Val Top-1: 23.78% | Val Top-5: 56.22%\n",
      "✓ New best model saved! Val Top-1: 23.78%\n",
      "Epoch 015/150 | Time: 81.7s | LR: 0.000997 | Train Loss: 3.6153 | Train Acc: 21.14% | Val Loss: 3.5154 | Val Top-1: 27.67% | Val Top-5: 58.78%\n",
      "✓ New best model saved! Val Top-1: 27.67%\n",
      "Epoch 016/150 | Time: 82.0s | LR: 0.000995 | Train Loss: 3.4837 | Train Acc: 25.59% | Val Loss: 3.3457 | Val Top-1: 32.44% | Val Top-5: 63.44%\n",
      "✓ New best model saved! Val Top-1: 32.44%\n",
      "Epoch 017/150 | Time: 82.0s | LR: 0.000994 | Train Loss: 3.3631 | Train Acc: 28.97% | Val Loss: 3.2313 | Val Top-1: 34.78% | Val Top-5: 67.78%\n",
      "✓ New best model saved! Val Top-1: 34.78%\n",
      "Epoch 018/150 | Time: 81.9s | LR: 0.000992 | Train Loss: 3.2413 | Train Acc: 30.27% | Val Loss: 3.1720 | Val Top-1: 34.78% | Val Top-5: 69.22%\n",
      "Epoch 019/150 | Time: 82.0s | LR: 0.000990 | Train Loss: 3.2159 | Train Acc: 30.86% | Val Loss: 3.0626 | Val Top-1: 39.89% | Val Top-5: 72.11%\n",
      "✓ New best model saved! Val Top-1: 39.89%\n",
      "Epoch 020/150 | Time: 106.0s | LR: 0.000987 | Train Loss: 2.9559 | Train Acc: 37.63% | Val Loss: 2.9836 | Val Top-1: 41.44% | Val Top-5: 75.89%\n",
      "✓ New best model saved! Val Top-1: 41.44%\n",
      "Epoch 021/150 | Time: 81.8s | LR: 0.000985 | Train Loss: 2.8418 | Train Acc: 42.64% | Val Loss: 2.8934 | Val Top-1: 42.67% | Val Top-5: 75.44%\n",
      "✓ New best model saved! Val Top-1: 42.67%\n",
      "Epoch 022/150 | Time: 81.4s | LR: 0.000982 | Train Loss: 2.8646 | Train Acc: 40.45% | Val Loss: 2.8260 | Val Top-1: 44.89% | Val Top-5: 77.00%\n",
      "✓ New best model saved! Val Top-1: 44.89%\n",
      "Epoch 023/150 | Time: 81.6s | LR: 0.000979 | Train Loss: 2.7639 | Train Acc: 39.23% | Val Loss: 2.8294 | Val Top-1: 46.11% | Val Top-5: 76.56%\n",
      "✓ New best model saved! Val Top-1: 46.11%\n",
      "Epoch 024/150 | Time: 81.9s | LR: 0.000976 | Train Loss: 2.6701 | Train Acc: 44.39% | Val Loss: 2.7403 | Val Top-1: 49.56% | Val Top-5: 78.00%\n",
      "✓ New best model saved! Val Top-1: 49.56%\n",
      "Epoch 025/150 | Time: 82.0s | LR: 0.000972 | Train Loss: 2.5765 | Train Acc: 45.38% | Val Loss: 2.6695 | Val Top-1: 50.33% | Val Top-5: 80.89%\n",
      "✓ New best model saved! Val Top-1: 50.33%\n",
      "Epoch 026/150 | Time: 82.2s | LR: 0.000968 | Train Loss: 2.5922 | Train Acc: 47.38% | Val Loss: 2.6619 | Val Top-1: 50.00% | Val Top-5: 80.78%\n",
      "Epoch 027/150 | Time: 81.7s | LR: 0.000964 | Train Loss: 2.4910 | Train Acc: 48.96% | Val Loss: 2.5997 | Val Top-1: 53.44% | Val Top-5: 82.56%\n",
      "✓ New best model saved! Val Top-1: 53.44%\n",
      "Epoch 028/150 | Time: 81.5s | LR: 0.000960 | Train Loss: 2.3617 | Train Acc: 50.62% | Val Loss: 2.5512 | Val Top-1: 54.44% | Val Top-5: 83.56%\n",
      "✓ New best model saved! Val Top-1: 54.44%\n",
      "Epoch 029/150 | Time: 82.1s | LR: 0.000955 | Train Loss: 2.3812 | Train Acc: 51.89% | Val Loss: 2.5486 | Val Top-1: 52.89% | Val Top-5: 83.89%\n",
      "Epoch 030/150 | Time: 104.7s | LR: 0.000950 | Train Loss: 2.3218 | Train Acc: 54.85% | Val Loss: 2.5224 | Val Top-1: 54.67% | Val Top-5: 83.22%\n",
      "✓ New best model saved! Val Top-1: 54.67%\n",
      "Epoch 031/150 | Time: 82.2s | LR: 0.000946 | Train Loss: 2.2559 | Train Acc: 54.28% | Val Loss: 2.4690 | Val Top-1: 55.56% | Val Top-5: 84.22%\n",
      "✓ New best model saved! Val Top-1: 55.56%\n",
      "Epoch 032/150 | Time: 82.2s | LR: 0.000940 | Train Loss: 2.2179 | Train Acc: 60.46% | Val Loss: 2.4405 | Val Top-1: 57.56% | Val Top-5: 84.67%\n",
      "✓ New best model saved! Val Top-1: 57.56%\n",
      "Epoch 033/150 | Time: 81.7s | LR: 0.000935 | Train Loss: 2.2601 | Train Acc: 56.96% | Val Loss: 2.4131 | Val Top-1: 57.67% | Val Top-5: 84.56%\n",
      "✓ New best model saved! Val Top-1: 57.67%\n",
      "Epoch 034/150 | Time: 81.7s | LR: 0.000929 | Train Loss: 2.0472 | Train Acc: 65.56% | Val Loss: 2.4024 | Val Top-1: 60.22% | Val Top-5: 84.78%\n",
      "✓ New best model saved! Val Top-1: 60.22%\n",
      "Epoch 035/150 | Time: 82.0s | LR: 0.000923 | Train Loss: 2.1539 | Train Acc: 55.78% | Val Loss: 2.3677 | Val Top-1: 61.44% | Val Top-5: 85.78%\n",
      "✓ New best model saved! Val Top-1: 61.44%\n",
      "Epoch 036/150 | Time: 81.3s | LR: 0.000917 | Train Loss: 2.1459 | Train Acc: 54.93% | Val Loss: 2.3650 | Val Top-1: 61.44% | Val Top-5: 85.56%\n",
      "Epoch 037/150 | Time: 81.1s | LR: 0.000911 | Train Loss: 2.0176 | Train Acc: 62.57% | Val Loss: 2.3375 | Val Top-1: 62.00% | Val Top-5: 86.22%\n",
      "✓ New best model saved! Val Top-1: 62.00%\n",
      "Epoch 038/150 | Time: 81.3s | LR: 0.000905 | Train Loss: 2.0001 | Train Acc: 58.31% | Val Loss: 2.3216 | Val Top-1: 62.00% | Val Top-5: 86.44%\n",
      "Epoch 039/150 | Time: 81.6s | LR: 0.000898 | Train Loss: 1.8999 | Train Acc: 63.90% | Val Loss: 2.2964 | Val Top-1: 60.78% | Val Top-5: 86.67%\n",
      "Epoch 040/150 | Time: 104.8s | LR: 0.000891 | Train Loss: 1.9626 | Train Acc: 66.23% | Val Loss: 2.2851 | Val Top-1: 62.78% | Val Top-5: 87.56%\n",
      "✓ New best model saved! Val Top-1: 62.78%\n",
      "Epoch 041/150 | Time: 81.4s | LR: 0.000884 | Train Loss: 2.0599 | Train Acc: 58.65% | Val Loss: 2.2411 | Val Top-1: 63.11% | Val Top-5: 87.67%\n",
      "✓ New best model saved! Val Top-1: 63.11%\n",
      "Epoch 042/150 | Time: 81.6s | LR: 0.000877 | Train Loss: 2.0069 | Train Acc: 61.05% | Val Loss: 2.2081 | Val Top-1: 66.00% | Val Top-5: 88.56%\n",
      "✓ New best model saved! Val Top-1: 66.00%\n",
      "Epoch 043/150 | Time: 82.1s | LR: 0.000869 | Train Loss: 1.8242 | Train Acc: 67.33% | Val Loss: 2.2088 | Val Top-1: 64.00% | Val Top-5: 89.22%\n",
      "Epoch 044/150 | Time: 81.8s | LR: 0.000861 | Train Loss: 1.9054 | Train Acc: 69.81% | Val Loss: 2.2103 | Val Top-1: 64.56% | Val Top-5: 87.33%\n",
      "Epoch 045/150 | Time: 82.0s | LR: 0.000854 | Train Loss: 1.9130 | Train Acc: 65.67% | Val Loss: 2.1612 | Val Top-1: 65.78% | Val Top-5: 88.11%\n",
      "Epoch 046/150 | Time: 81.8s | LR: 0.000846 | Train Loss: 1.9062 | Train Acc: 66.83% | Val Loss: 2.2418 | Val Top-1: 64.67% | Val Top-5: 87.89%\n",
      "Epoch 047/150 | Time: 81.7s | LR: 0.000837 | Train Loss: 1.9286 | Train Acc: 65.92% | Val Loss: 2.1979 | Val Top-1: 66.22% | Val Top-5: 88.11%\n",
      "✓ New best model saved! Val Top-1: 66.22%\n",
      "Epoch 048/150 | Time: 81.6s | LR: 0.000829 | Train Loss: 1.7570 | Train Acc: 76.32% | Val Loss: 2.2066 | Val Top-1: 65.67% | Val Top-5: 87.89%\n",
      "Epoch 049/150 | Time: 81.4s | LR: 0.000820 | Train Loss: 1.7734 | Train Acc: 68.86% | Val Loss: 2.1641 | Val Top-1: 67.11% | Val Top-5: 87.78%\n",
      "✓ New best model saved! Val Top-1: 67.11%\n",
      "Epoch 050/150 | Time: 104.7s | LR: 0.000812 | Train Loss: 1.8152 | Train Acc: 71.17% | Val Loss: 2.2188 | Val Top-1: 64.22% | Val Top-5: 88.44%\n",
      "Epoch 051/150 | Time: 81.3s | LR: 0.000803 | Train Loss: 1.7649 | Train Acc: 64.66% | Val Loss: 2.1755 | Val Top-1: 65.67% | Val Top-5: 87.11%\n",
      "Epoch 052/150 | Time: 81.8s | LR: 0.000794 | Train Loss: 1.7843 | Train Acc: 65.16% | Val Loss: 2.1315 | Val Top-1: 67.78% | Val Top-5: 88.67%\n",
      "✓ New best model saved! Val Top-1: 67.78%\n",
      "Epoch 053/150 | Time: 81.3s | LR: 0.000785 | Train Loss: 1.6816 | Train Acc: 65.73% | Val Loss: 2.1735 | Val Top-1: 65.44% | Val Top-5: 86.89%\n",
      "Epoch 054/150 | Time: 81.1s | LR: 0.000775 | Train Loss: 1.6984 | Train Acc: 72.38% | Val Loss: 2.1424 | Val Top-1: 66.67% | Val Top-5: 87.89%\n",
      "Epoch 055/150 | Time: 81.2s | LR: 0.000766 | Train Loss: 1.6331 | Train Acc: 73.65% | Val Loss: 2.1239 | Val Top-1: 68.22% | Val Top-5: 88.89%\n",
      "✓ New best model saved! Val Top-1: 68.22%\n",
      "Epoch 056/150 | Time: 81.5s | LR: 0.000756 | Train Loss: 1.6923 | Train Acc: 68.15% | Val Loss: 2.1061 | Val Top-1: 68.78% | Val Top-5: 89.00%\n",
      "✓ New best model saved! Val Top-1: 68.78%\n",
      "Epoch 057/150 | Time: 81.4s | LR: 0.000747 | Train Loss: 1.6590 | Train Acc: 70.35% | Val Loss: 2.1366 | Val Top-1: 66.67% | Val Top-5: 88.22%\n",
      "Epoch 058/150 | Time: 81.7s | LR: 0.000737 | Train Loss: 1.7679 | Train Acc: 71.65% | Val Loss: 2.1406 | Val Top-1: 67.22% | Val Top-5: 88.33%\n",
      "Epoch 059/150 | Time: 82.4s | LR: 0.000727 | Train Loss: 1.6742 | Train Acc: 74.55% | Val Loss: 2.1151 | Val Top-1: 68.00% | Val Top-5: 89.44%\n",
      "Epoch 060/150 | Time: 105.4s | LR: 0.000717 | Train Loss: 1.7000 | Train Acc: 66.18% | Val Loss: 2.0869 | Val Top-1: 66.89% | Val Top-5: 90.33%\n",
      "Epoch 061/150 | Time: 81.6s | LR: 0.000707 | Train Loss: 1.7220 | Train Acc: 67.81% | Val Loss: 2.1196 | Val Top-1: 66.44% | Val Top-5: 88.44%\n",
      "Epoch 062/150 | Time: 81.6s | LR: 0.000697 | Train Loss: 1.5695 | Train Acc: 75.37% | Val Loss: 2.0425 | Val Top-1: 70.00% | Val Top-5: 89.67%\n",
      "✓ New best model saved! Val Top-1: 70.00%\n",
      "Epoch 063/150 | Time: 82.2s | LR: 0.000686 | Train Loss: 1.6066 | Train Acc: 68.57% | Val Loss: 2.0766 | Val Top-1: 67.22% | Val Top-5: 89.78%\n",
      "Epoch 064/150 | Time: 81.9s | LR: 0.000676 | Train Loss: 1.7075 | Train Acc: 77.68% | Val Loss: 2.0306 | Val Top-1: 70.00% | Val Top-5: 90.56%\n",
      "Epoch 065/150 | Time: 81.8s | LR: 0.000665 | Train Loss: 1.6468 | Train Acc: 73.90% | Val Loss: 2.0501 | Val Top-1: 69.44% | Val Top-5: 90.11%\n",
      "Epoch 066/150 | Time: 81.5s | LR: 0.000655 | Train Loss: 1.6536 | Train Acc: 74.83% | Val Loss: 2.0472 | Val Top-1: 69.89% | Val Top-5: 89.89%\n",
      "Epoch 067/150 | Time: 81.5s | LR: 0.000644 | Train Loss: 1.7143 | Train Acc: 73.25% | Val Loss: 2.0487 | Val Top-1: 69.67% | Val Top-5: 89.33%\n",
      "Epoch 068/150 | Time: 81.5s | LR: 0.000633 | Train Loss: 1.5240 | Train Acc: 68.26% | Val Loss: 2.0350 | Val Top-1: 70.56% | Val Top-5: 89.44%\n",
      "✓ New best model saved! Val Top-1: 70.56%\n",
      "Epoch 069/150 | Time: 82.0s | LR: 0.000622 | Train Loss: 1.6235 | Train Acc: 76.18% | Val Loss: 2.0059 | Val Top-1: 70.67% | Val Top-5: 90.67%\n",
      "✓ New best model saved! Val Top-1: 70.67%\n",
      "Epoch 070/150 | Time: 105.3s | LR: 0.000611 | Train Loss: 1.4791 | Train Acc: 76.92% | Val Loss: 2.0473 | Val Top-1: 69.67% | Val Top-5: 89.78%\n",
      "Epoch 071/150 | Time: 81.8s | LR: 0.000600 | Train Loss: 1.5823 | Train Acc: 71.76% | Val Loss: 2.0205 | Val Top-1: 70.33% | Val Top-5: 89.67%\n",
      "Epoch 072/150 | Time: 81.9s | LR: 0.000589 | Train Loss: 1.6289 | Train Acc: 69.76% | Val Loss: 2.0501 | Val Top-1: 68.89% | Val Top-5: 89.44%\n",
      "Epoch 073/150 | Time: 82.1s | LR: 0.000578 | Train Loss: 1.5938 | Train Acc: 71.59% | Val Loss: 1.9817 | Val Top-1: 70.89% | Val Top-5: 90.89%\n",
      "✓ New best model saved! Val Top-1: 70.89%\n",
      "Epoch 074/150 | Time: 82.2s | LR: 0.000567 | Train Loss: 1.5870 | Train Acc: 73.73% | Val Loss: 2.0048 | Val Top-1: 71.11% | Val Top-5: 89.00%\n",
      "✓ New best model saved! Val Top-1: 71.11%\n",
      "Epoch 075/150 | Time: 82.2s | LR: 0.000556 | Train Loss: 1.5746 | Train Acc: 73.59% | Val Loss: 1.9923 | Val Top-1: 71.11% | Val Top-5: 89.56%\n",
      "Epoch 076/150 | Time: 82.1s | LR: 0.000545 | Train Loss: 1.5937 | Train Acc: 72.41% | Val Loss: 2.0485 | Val Top-1: 69.44% | Val Top-5: 90.11%\n",
      "Epoch 077/150 | Time: 82.2s | LR: 0.000534 | Train Loss: 1.4946 | Train Acc: 71.62% | Val Loss: 1.9772 | Val Top-1: 72.11% | Val Top-5: 90.22%\n",
      "✓ New best model saved! Val Top-1: 72.11%\n",
      "Epoch 078/150 | Time: 82.1s | LR: 0.000522 | Train Loss: 1.4901 | Train Acc: 72.38% | Val Loss: 2.0034 | Val Top-1: 70.56% | Val Top-5: 90.11%\n",
      "Epoch 079/150 | Time: 81.5s | LR: 0.000511 | Train Loss: 1.6036 | Train Acc: 69.95% | Val Loss: 1.9890 | Val Top-1: 71.33% | Val Top-5: 90.78%\n",
      "Epoch 080/150 | Time: 105.0s | LR: 0.000500 | Train Loss: 1.5538 | Train Acc: 72.94% | Val Loss: 1.9791 | Val Top-1: 72.56% | Val Top-5: 89.44%\n",
      "✓ New best model saved! Val Top-1: 72.56%\n",
      "Epoch 081/150 | Time: 82.1s | LR: 0.000489 | Train Loss: 1.5029 | Train Acc: 74.15% | Val Loss: 1.9990 | Val Top-1: 72.89% | Val Top-5: 90.00%\n",
      "✓ New best model saved! Val Top-1: 72.89%\n",
      "Epoch 082/150 | Time: 81.4s | LR: 0.000478 | Train Loss: 1.4987 | Train Acc: 75.93% | Val Loss: 1.9562 | Val Top-1: 73.89% | Val Top-5: 90.11%\n",
      "✓ New best model saved! Val Top-1: 73.89%\n",
      "Epoch 083/150 | Time: 81.6s | LR: 0.000466 | Train Loss: 1.6561 | Train Acc: 69.84% | Val Loss: 1.9613 | Val Top-1: 73.56% | Val Top-5: 90.89%\n",
      "Epoch 084/150 | Time: 82.2s | LR: 0.000455 | Train Loss: 1.4349 | Train Acc: 80.44% | Val Loss: 1.9727 | Val Top-1: 72.67% | Val Top-5: 90.56%\n",
      "Epoch 085/150 | Time: 81.6s | LR: 0.000444 | Train Loss: 1.6541 | Train Acc: 70.66% | Val Loss: 1.9805 | Val Top-1: 72.78% | Val Top-5: 91.33%\n",
      "Epoch 086/150 | Time: 81.8s | LR: 0.000433 | Train Loss: 1.4780 | Train Acc: 74.30% | Val Loss: 1.9809 | Val Top-1: 72.33% | Val Top-5: 90.33%\n",
      "Epoch 087/150 | Time: 82.4s | LR: 0.000422 | Train Loss: 1.5401 | Train Acc: 68.07% | Val Loss: 1.9521 | Val Top-1: 73.11% | Val Top-5: 90.89%\n",
      "Epoch 088/150 | Time: 81.6s | LR: 0.000411 | Train Loss: 1.5513 | Train Acc: 72.15% | Val Loss: 1.9987 | Val Top-1: 70.44% | Val Top-5: 90.56%\n",
      "Epoch 089/150 | Time: 81.2s | LR: 0.000400 | Train Loss: 1.4573 | Train Acc: 71.79% | Val Loss: 1.9492 | Val Top-1: 72.44% | Val Top-5: 91.44%\n",
      "Epoch 090/150 | Time: 104.7s | LR: 0.000389 | Train Loss: 1.5873 | Train Acc: 66.80% | Val Loss: 1.9799 | Val Top-1: 73.33% | Val Top-5: 90.56%\n",
      "Epoch 091/150 | Time: 82.1s | LR: 0.000378 | Train Loss: 1.4721 | Train Acc: 75.70% | Val Loss: 1.9592 | Val Top-1: 72.78% | Val Top-5: 90.33%\n",
      "Epoch 092/150 | Time: 81.8s | LR: 0.000367 | Train Loss: 1.4662 | Train Acc: 74.38% | Val Loss: 1.9394 | Val Top-1: 73.67% | Val Top-5: 91.00%\n",
      "Epoch 093/150 | Time: 81.3s | LR: 0.000356 | Train Loss: 1.6351 | Train Acc: 68.63% | Val Loss: 1.9372 | Val Top-1: 73.44% | Val Top-5: 91.56%\n",
      "Epoch 094/150 | Time: 81.8s | LR: 0.000345 | Train Loss: 1.5393 | Train Acc: 69.17% | Val Loss: 1.9335 | Val Top-1: 73.78% | Val Top-5: 91.56%\n",
      "Epoch 095/150 | Time: 82.1s | LR: 0.000335 | Train Loss: 1.4501 | Train Acc: 81.03% | Val Loss: 1.9568 | Val Top-1: 72.89% | Val Top-5: 90.33%\n",
      "Epoch 096/150 | Time: 81.9s | LR: 0.000324 | Train Loss: 1.4319 | Train Acc: 75.14% | Val Loss: 1.9474 | Val Top-1: 73.00% | Val Top-5: 91.89%\n",
      "Epoch 097/150 | Time: 81.3s | LR: 0.000314 | Train Loss: 1.4790 | Train Acc: 73.11% | Val Loss: 1.9342 | Val Top-1: 74.67% | Val Top-5: 91.11%\n",
      "✓ New best model saved! Val Top-1: 74.67%\n",
      "Epoch 098/150 | Time: 82.1s | LR: 0.000303 | Train Loss: 1.4538 | Train Acc: 75.17% | Val Loss: 1.9504 | Val Top-1: 72.89% | Val Top-5: 90.89%\n",
      "Epoch 099/150 | Time: 81.4s | LR: 0.000293 | Train Loss: 1.5624 | Train Acc: 74.55% | Val Loss: 1.9538 | Val Top-1: 72.67% | Val Top-5: 90.89%\n",
      "Epoch 100/150 | Time: 105.3s | LR: 0.000283 | Train Loss: 1.6052 | Train Acc: 71.93% | Val Loss: 1.9627 | Val Top-1: 72.89% | Val Top-5: 90.44%\n",
      "Epoch 101/150 | Time: 82.5s | LR: 0.000273 | Train Loss: 1.5043 | Train Acc: 77.59% | Val Loss: 1.9427 | Val Top-1: 74.22% | Val Top-5: 91.11%\n",
      "Epoch 102/150 | Time: 82.6s | LR: 0.000263 | Train Loss: 1.4356 | Train Acc: 77.71% | Val Loss: 1.9366 | Val Top-1: 75.22% | Val Top-5: 90.22%\n",
      "✓ New best model saved! Val Top-1: 75.22%\n",
      "Epoch 103/150 | Time: 82.0s | LR: 0.000253 | Train Loss: 1.5116 | Train Acc: 75.06% | Val Loss: 1.9341 | Val Top-1: 73.33% | Val Top-5: 91.00%\n",
      "Epoch 104/150 | Time: 82.1s | LR: 0.000244 | Train Loss: 1.4885 | Train Acc: 73.48% | Val Loss: 1.9387 | Val Top-1: 73.56% | Val Top-5: 91.11%\n",
      "Epoch 105/150 | Time: 82.5s | LR: 0.000234 | Train Loss: 1.5421 | Train Acc: 72.69% | Val Loss: 1.9463 | Val Top-1: 75.44% | Val Top-5: 91.11%\n",
      "✓ New best model saved! Val Top-1: 75.44%\n",
      "Epoch 106/150 | Time: 81.6s | LR: 0.000225 | Train Loss: 1.5049 | Train Acc: 67.36% | Val Loss: 1.9314 | Val Top-1: 74.33% | Val Top-5: 91.22%\n",
      "Epoch 107/150 | Time: 82.5s | LR: 0.000215 | Train Loss: 1.5704 | Train Acc: 72.83% | Val Loss: 1.9209 | Val Top-1: 74.56% | Val Top-5: 91.89%\n",
      "Epoch 108/150 | Time: 81.7s | LR: 0.000206 | Train Loss: 1.4178 | Train Acc: 77.54% | Val Loss: 1.9140 | Val Top-1: 74.11% | Val Top-5: 91.67%\n",
      "Epoch 109/150 | Time: 82.5s | LR: 0.000197 | Train Loss: 1.4726 | Train Acc: 79.28% | Val Loss: 1.9248 | Val Top-1: 75.44% | Val Top-5: 91.89%\n",
      "Epoch 110/150 | Time: 106.2s | LR: 0.000188 | Train Loss: 1.5711 | Train Acc: 72.10% | Val Loss: 1.9333 | Val Top-1: 74.56% | Val Top-5: 91.56%\n",
      "Epoch 111/150 | Time: 81.9s | LR: 0.000180 | Train Loss: 1.4561 | Train Acc: 72.41% | Val Loss: 1.9413 | Val Top-1: 74.44% | Val Top-5: 91.22%\n",
      "Epoch 112/150 | Time: 81.7s | LR: 0.000171 | Train Loss: 1.4407 | Train Acc: 74.38% | Val Loss: 1.9011 | Val Top-1: 75.56% | Val Top-5: 91.67%\n",
      "✓ New best model saved! Val Top-1: 75.56%\n",
      "Epoch 113/150 | Time: 82.1s | LR: 0.000163 | Train Loss: 1.4533 | Train Acc: 71.51% | Val Loss: 1.9110 | Val Top-1: 75.00% | Val Top-5: 91.00%\n",
      "Epoch 114/150 | Time: 82.4s | LR: 0.000154 | Train Loss: 1.4156 | Train Acc: 76.61% | Val Loss: 1.9055 | Val Top-1: 76.11% | Val Top-5: 91.11%\n",
      "✓ New best model saved! Val Top-1: 76.11%\n",
      "Epoch 115/150 | Time: 82.2s | LR: 0.000146 | Train Loss: 1.4611 | Train Acc: 71.17% | Val Loss: 1.8982 | Val Top-1: 76.00% | Val Top-5: 91.56%\n",
      "Epoch 116/150 | Time: 82.3s | LR: 0.000139 | Train Loss: 1.4373 | Train Acc: 69.25% | Val Loss: 1.9090 | Val Top-1: 75.44% | Val Top-5: 91.78%\n",
      "Epoch 117/150 | Time: 82.4s | LR: 0.000131 | Train Loss: 1.3578 | Train Acc: 73.22% | Val Loss: 1.9006 | Val Top-1: 75.00% | Val Top-5: 91.56%\n",
      "Epoch 118/150 | Time: 82.4s | LR: 0.000123 | Train Loss: 1.5272 | Train Acc: 74.01% | Val Loss: 1.9262 | Val Top-1: 75.44% | Val Top-5: 91.78%\n",
      "Epoch 119/150 | Time: 82.4s | LR: 0.000116 | Train Loss: 1.4225 | Train Acc: 77.28% | Val Loss: 1.9112 | Val Top-1: 74.67% | Val Top-5: 91.78%\n",
      "Epoch 120/150 | Time: 106.2s | LR: 0.000109 | Train Loss: 1.3974 | Train Acc: 75.82% | Val Loss: 1.8988 | Val Top-1: 75.11% | Val Top-5: 91.67%\n",
      "Epoch 121/150 | Time: 82.4s | LR: 0.000102 | Train Loss: 1.3890 | Train Acc: 75.42% | Val Loss: 1.8991 | Val Top-1: 75.78% | Val Top-5: 92.11%\n",
      "Epoch 122/150 | Time: 82.4s | LR: 0.000095 | Train Loss: 1.4833 | Train Acc: 73.14% | Val Loss: 1.8889 | Val Top-1: 75.22% | Val Top-5: 91.78%\n",
      "Epoch 123/150 | Time: 82.2s | LR: 0.000089 | Train Loss: 1.4160 | Train Acc: 71.84% | Val Loss: 1.9035 | Val Top-1: 75.11% | Val Top-5: 92.33%\n",
      "Epoch 124/150 | Time: 82.6s | LR: 0.000083 | Train Loss: 1.5002 | Train Acc: 74.35% | Val Loss: 1.8904 | Val Top-1: 75.11% | Val Top-5: 92.00%\n",
      "Epoch 125/150 | Time: 82.6s | LR: 0.000077 | Train Loss: 1.4284 | Train Acc: 74.69% | Val Loss: 1.8996 | Val Top-1: 75.33% | Val Top-5: 91.67%\n",
      "Epoch 126/150 | Time: 82.3s | LR: 0.000071 | Train Loss: 1.3318 | Train Acc: 76.44% | Val Loss: 1.8994 | Val Top-1: 74.22% | Val Top-5: 91.33%\n",
      "Epoch 127/150 | Time: 82.2s | LR: 0.000065 | Train Loss: 1.4968 | Train Acc: 71.87% | Val Loss: 1.8993 | Val Top-1: 75.33% | Val Top-5: 91.89%\n",
      "Epoch 128/150 | Time: 82.2s | LR: 0.000060 | Train Loss: 1.4180 | Train Acc: 69.84% | Val Loss: 1.9045 | Val Top-1: 75.44% | Val Top-5: 91.78%\n",
      "Epoch 129/150 | Time: 82.2s | LR: 0.000054 | Train Loss: 1.3364 | Train Acc: 74.18% | Val Loss: 1.8981 | Val Top-1: 75.44% | Val Top-5: 91.89%\n",
      "Epoch 130/150 | Time: 105.9s | LR: 0.000050 | Train Loss: 1.4126 | Train Acc: 76.35% | Val Loss: 1.9026 | Val Top-1: 75.56% | Val Top-5: 91.78%\n",
      "Epoch 131/150 | Time: 82.3s | LR: 0.000045 | Train Loss: 1.3460 | Train Acc: 77.25% | Val Loss: 1.9006 | Val Top-1: 75.67% | Val Top-5: 91.89%\n",
      "Epoch 132/150 | Time: 82.1s | LR: 0.000040 | Train Loss: 1.4536 | Train Acc: 73.22% | Val Loss: 1.8967 | Val Top-1: 76.67% | Val Top-5: 91.44%\n",
      "✓ New best model saved! Val Top-1: 76.67%\n",
      "Epoch 133/150 | Time: 82.1s | LR: 0.000036 | Train Loss: 1.4321 | Train Acc: 78.44% | Val Loss: 1.8950 | Val Top-1: 75.89% | Val Top-5: 91.67%\n",
      "Epoch 134/150 | Time: 82.2s | LR: 0.000032 | Train Loss: 1.3733 | Train Acc: 70.10% | Val Loss: 1.8775 | Val Top-1: 76.00% | Val Top-5: 91.67%\n",
      "Epoch 135/150 | Time: 82.4s | LR: 0.000028 | Train Loss: 1.3585 | Train Acc: 71.31% | Val Loss: 1.9057 | Val Top-1: 76.00% | Val Top-5: 91.78%\n",
      "Epoch 136/150 | Time: 82.6s | LR: 0.000024 | Train Loss: 1.3678 | Train Acc: 75.25% | Val Loss: 1.8848 | Val Top-1: 76.33% | Val Top-5: 91.78%\n",
      "Epoch 137/150 | Time: 82.4s | LR: 0.000021 | Train Loss: 1.3589 | Train Acc: 72.10% | Val Loss: 1.8842 | Val Top-1: 75.78% | Val Top-5: 91.78%\n",
      "Epoch 138/150 | Time: 82.3s | LR: 0.000018 | Train Loss: 1.3773 | Train Acc: 73.31% | Val Loss: 1.8981 | Val Top-1: 75.78% | Val Top-5: 92.00%\n",
      "Epoch 139/150 | Time: 82.7s | LR: 0.000015 | Train Loss: 1.3724 | Train Acc: 74.27% | Val Loss: 1.9005 | Val Top-1: 76.22% | Val Top-5: 92.11%\n",
      "Epoch 140/150 | Time: 106.8s | LR: 0.000013 | Train Loss: 1.4188 | Train Acc: 73.53% | Val Loss: 1.9031 | Val Top-1: 75.78% | Val Top-5: 91.89%\n",
      "Epoch 141/150 | Time: 82.0s | LR: 0.000010 | Train Loss: 1.4578 | Train Acc: 68.97% | Val Loss: 1.8919 | Val Top-1: 76.00% | Val Top-5: 91.67%\n",
      "Epoch 142/150 | Time: 81.8s | LR: 0.000008 | Train Loss: 1.3856 | Train Acc: 77.03% | Val Loss: 1.8841 | Val Top-1: 76.22% | Val Top-5: 91.78%\n",
      "Epoch 143/150 | Time: 82.5s | LR: 0.000006 | Train Loss: 1.3880 | Train Acc: 73.39% | Val Loss: 1.8983 | Val Top-1: 75.78% | Val Top-5: 91.56%\n",
      "Epoch 144/150 | Time: 82.8s | LR: 0.000005 | Train Loss: 1.4132 | Train Acc: 73.59% | Val Loss: 1.9045 | Val Top-1: 76.00% | Val Top-5: 92.11%\n",
      "Epoch 145/150 | Time: 82.7s | LR: 0.000003 | Train Loss: 1.5058 | Train Acc: 77.96% | Val Loss: 1.8937 | Val Top-1: 75.67% | Val Top-5: 91.89%\n",
      "Epoch 146/150 | Time: 82.5s | LR: 0.000002 | Train Loss: 1.4654 | Train Acc: 69.17% | Val Loss: 1.8973 | Val Top-1: 75.89% | Val Top-5: 91.67%\n",
      "Epoch 147/150 | Time: 82.3s | LR: 0.000001 | Train Loss: 1.4311 | Train Acc: 74.46% | Val Loss: 1.8913 | Val Top-1: 76.33% | Val Top-5: 91.56%\n",
      "Epoch 148/150 | Time: 82.2s | LR: 0.000001 | Train Loss: 1.3464 | Train Acc: 75.99% | Val Loss: 1.8829 | Val Top-1: 75.89% | Val Top-5: 91.78%\n",
      "Epoch 149/150 | Time: 82.1s | LR: 0.000000 | Train Loss: 1.3202 | Train Acc: 73.42% | Val Loss: 1.8860 | Val Top-1: 75.44% | Val Top-5: 91.89%\n",
      "Epoch 150/150 | Time: 105.8s | LR: 0.000000 | Train Loss: 1.2632 | Train Acc: 77.90% | Val Loss: 1.8773 | Val Top-1: 75.78% | Val Top-5: 91.78%\n",
      "\n",
      "======================================================================\n",
      "EVALUATING ON TEST SET\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/513600044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;31m# Load best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_tt_stgcn.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                         )\n\u001b[1;32m   1469\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m                 return _load(\n\u001b[1;32m   1472\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Paths\n",
    "FEATURES_DIR = '/kaggle/input/msegcn-individual/MSE_GCN_features_individual'\n",
    "JSON_PATH = '/kaggle/input/json-files/nslt_300.json'\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 24  # Increased for better gradient estimates\n",
    "EPOCHS = 150  # More epochs with better regularization\n",
    "BASE_CHANNELS = 32  # Lightweight\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.05\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MIXUP_ALPHA = 0.2\n",
    "\n",
    "print(f\"Configuration loaded. Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA AND CREATE SPLITS\n",
    "# ============================================================================\n",
    "print(\"\\nLoading JSON data...\")\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "train_ids, val_ids, test_ids = [], [], []\n",
    "labels_dict = {}\n",
    "\n",
    "unique_labels = sorted({info['action'][0] for info in json_data.values()})\n",
    "class_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "for video_id, info in json_data.items():\n",
    "    file_path = os.path.join(FEATURES_DIR, f\"{video_id}.npz\")\n",
    "    if os.path.exists(file_path):\n",
    "        original_label = info['action'][0]\n",
    "        labels_dict[video_id] = class_to_idx[original_label]\n",
    "        \n",
    "        if info['subset'] == 'train':\n",
    "            train_ids.append(video_id)\n",
    "        elif info['subset'] == 'val':\n",
    "            val_ids.append(video_id)\n",
    "        elif info['subset'] == 'test':\n",
    "            test_ids.append(video_id)\n",
    "\n",
    "print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# ============================================================================\n",
    "print(\"\\nCreating datasets with augmentation...\")\n",
    "\n",
    "# Augmentation for training\n",
    "augmentation = SignLanguageAugmentation(\n",
    "    shear_range=0.1,\n",
    "    rotate_range=20,\n",
    "    scale_range=(0.9, 1.1),\n",
    "    temporal_crop_ratio=0.9,\n",
    "    spatial_drop_prob=0.1,\n",
    "    temporal_mask_prob=0.15\n",
    ")\n",
    "\n",
    "train_dataset = EnhancedSignLanguageDataset(\n",
    "    train_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=augmentation, training=True\n",
    ")\n",
    "\n",
    "val_dataset = EnhancedSignLanguageDataset(\n",
    "    val_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=None, training=False\n",
    ")\n",
    "\n",
    "test_dataset = EnhancedSignLanguageDataset(\n",
    "    test_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=None, training=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Dataloaders created. Train batches: {len(train_loader)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INSTANTIATE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\nInitializing TT-STGCN model...\")\n",
    "model = TT_STGCN(num_classes=num_classes, base_ch=BASE_CHANNELS).to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "print(\"\\nSetting up training components...\")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# Optimizer: AdamW with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Scheduler: Cosine annealing with warmup\n",
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, warmup_epochs=10, max_epochs=EPOCHS)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "PATIENCE = 50  # Early stopping patience\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch_advanced(\n",
    "        model, train_loader, optimizer, criterion, DEVICE,\n",
    "        use_mixup=True, mixup_alpha=MIXUP_ALPHA\n",
    "    )\n",
    "    \n",
    "    # Validate (with TTA every 10 epochs for speed)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        val_loss, val_top1, val_top5 = validate_with_tta(\n",
    "            model, val_loader, criterion, DEVICE, num_tta=5\n",
    "        )\n",
    "    else:\n",
    "        val_loss, val_top1, val_top5 = validate_with_tta(\n",
    "            model, val_loader, criterion, DEVICE, num_tta=1\n",
    "        )\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1:03d}/{EPOCHS} | Time: {epoch_time:.1f}s | \"\n",
    "          f\"LR: {current_lr:.6f} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Top-1: {val_top1:.2f}% | Val Top-5: {val_top5:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_top1 > best_val_acc:\n",
    "        best_val_acc = val_top1\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_top1,\n",
    "        }, 'best_tt_stgcn.pth')\n",
    "        print(f\"✓ New best model saved! Val Top-1: {best_val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL EVALUATION ON TEST SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_tt_stgcn.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_loss, test_top1, test_top5 = validate_with_tta(\n",
    "    model, test_loader, criterion, DEVICE, num_tta=10\n",
    ")\n",
    "\n",
    "print(f\"\\nFINAL TEST RESULTS:\")\n",
    "print(f\"  Test Top-1 Accuracy: {test_top1:.2f}%\")\n",
    "print(f\"  Test Top-5 Accuracy: {test_top5:.2f}%\")\n",
    "print(f\"  Best Val Top-1 Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"\\n✓ Training complete! Model saved as 'best_tt_stgcn.pth'\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE PREDICTIONS FOR SUBMISSION\n",
    "# ============================================================================\n",
    "print(\"\\nGenerating predictions for submission...\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_video_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(test_dataset)), desc=\"Predicting\"):\n",
    "        joint, bone, _ = test_dataset[idx]\n",
    "        joint = joint.unsqueeze(0).to(DEVICE)\n",
    "        bone = bone.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # TTA for final predictions\n",
    "        tta_preds = []\n",
    "        for _ in range(10):\n",
    "            output = model(joint, bone)\n",
    "            tta_preds.append(F.softmax(output, dim=1))\n",
    "        \n",
    "        pred = torch.stack(tta_preds).mean(0)\n",
    "        pred_class = pred.argmax(1).item()\n",
    "        \n",
    "        all_preds.append(pred_class)\n",
    "        all_video_ids.append(test_ids[idx])\n",
    "\n",
    "# Save predictions\n",
    "predictions_dict = {vid: pred for vid, pred in zip(all_video_ids, all_preds)}\n",
    "with open('test_predictions.json', 'w') as f:\n",
    "    json.dump(predictions_dict, f, indent=2)\n",
    "\n",
    "print(\"✓ Predictions saved to 'test_predictions.json'\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL DONE! 🎉\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Test Accuracy on 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T07:48:20.755553Z",
     "iopub.status.busy": "2025-11-02T07:48:20.754960Z",
     "iopub.status.idle": "2025-11-02T07:48:42.767170Z",
     "shell.execute_reply": "2025-11-02T07:48:42.766433Z",
     "shell.execute_reply.started": "2025-11-02T07:48:20.755525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration loaded. Batch size: 24, Epochs: 150\n",
      "\n",
      "Loading JSON data...\n",
      "Number of classes: 300\n",
      "Train: 3548, Val: 900, Test: 668\n",
      "\n",
      "Creating datasets with augmentation...\n",
      "Dataloaders created. Train batches: 148\n",
      "\n",
      "Initializing TT-STGCN model...\n",
      "Total trainable parameters: 1,136,005\n",
      "\n",
      "Setting up training components...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Paths\n",
    "FEATURES_DIR = '/kaggle/input/msegcn-individual/MSE_GCN_features_individual'\n",
    "JSON_PATH = '/kaggle/input/json-files/nslt_300.json'\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 24  # Increased for better gradient estimates\n",
    "EPOCHS = 150  # More epochs with better regularization\n",
    "BASE_CHANNELS = 32  # Lightweight\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.05\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MIXUP_ALPHA = 0.2\n",
    "\n",
    "print(f\"Configuration loaded. Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA AND CREATE SPLITS\n",
    "# ============================================================================\n",
    "print(\"\\nLoading JSON data...\")\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "train_ids, val_ids, test_ids = [], [], []\n",
    "labels_dict = {}\n",
    "\n",
    "unique_labels = sorted({info['action'][0] for info in json_data.values()})\n",
    "class_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "for video_id, info in json_data.items():\n",
    "    file_path = os.path.join(FEATURES_DIR, f\"{video_id}.npz\")\n",
    "    if os.path.exists(file_path):\n",
    "        original_label = info['action'][0]\n",
    "        labels_dict[video_id] = class_to_idx[original_label]\n",
    "        \n",
    "        if info['subset'] == 'train':\n",
    "            train_ids.append(video_id)\n",
    "        elif info['subset'] == 'val':\n",
    "            val_ids.append(video_id)\n",
    "        elif info['subset'] == 'test':\n",
    "            test_ids.append(video_id)\n",
    "\n",
    "print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")\n",
    "\n",
    "print(\"\\nCreating datasets with augmentation...\")\n",
    "\n",
    "# Augmentation for training\n",
    "augmentation = SignLanguageAugmentation(\n",
    "    shear_range=0.1,\n",
    "    rotate_range=20,\n",
    "    scale_range=(0.9, 1.1),\n",
    "    temporal_crop_ratio=0.9,\n",
    "    spatial_drop_prob=0.1,\n",
    "    temporal_mask_prob=0.15\n",
    ")\n",
    "\n",
    "train_dataset = EnhancedSignLanguageDataset(\n",
    "    train_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=augmentation, training=True\n",
    ")\n",
    "\n",
    "val_dataset = EnhancedSignLanguageDataset(\n",
    "    val_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=None, training=False\n",
    ")\n",
    "\n",
    "test_dataset = EnhancedSignLanguageDataset(\n",
    "    test_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=None, training=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Dataloaders created. Train batches: {len(train_loader)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INSTANTIATE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\nInitializing TT-STGCN model...\")\n",
    "model = TT_STGCN(num_classes=num_classes, base_ch=BASE_CHANNELS).to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "print(\"\\nSetting up training components...\")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# Optimizer: AdamW with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Scheduler: Cosine annealing with warmup\n",
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, warmup_epochs=10, max_epochs=EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T07:53:21.978388Z",
     "iopub.status.busy": "2025-11-02T07:53:21.977803Z",
     "iopub.status.idle": "2025-11-02T07:53:22.216152Z",
     "shell.execute_reply": "2025-11-02T07:53:22.215552Z",
     "shell.execute_reply.started": "2025-11-02T07:53:21.978349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded best model from: /kaggle/input/ttstgcn/other/default/1/best_tt_stgcn.pth\n",
      "✅ Loaded 668 test samples.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "BEST_MODEL_PATH = \"/kaggle/input/ttstgcn/other/default/1/best_tt_stgcn.pth\" # Change path if needed\n",
    "\n",
    "\n",
    "\n",
    "model = TT_STGCN(num_classes=300, base_ch=BASE_CHANNELS).to(DEVICE)\n",
    "checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "# If checkpoint contains model_state_dict, extract it\n",
    "if \"model_state_dict\" in checkpoint:\n",
    "    state_dict = checkpoint[\"model_state_dict\"]\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ Loaded best model from: {BEST_MODEL_PATH}\")\n",
    "\n",
    "\n",
    "test_augmentation = SignLanguageAugmentation() # no effect since training=False\n",
    "test_dataset = EnhancedSignLanguageDataset(\n",
    "test_ids,\n",
    "labels_dict,\n",
    "FEATURES_DIR,\n",
    "seq_len=SEQ_LEN,\n",
    "augmentation=test_augmentation,\n",
    "training=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"✅ Loaded {len(test_dataset)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T07:54:46.962951Z",
     "iopub.status.busy": "2025-11-02T07:54:46.962237Z",
     "iopub.status.idle": "2025-11-02T07:54:52.403285Z",
     "shell.execute_reply": "2025-11-02T07:54:52.402535Z",
     "shell.execute_reply.started": "2025-11-02T07:54:46.962923Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 21/21 [00:05<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Test Accuracy (Top-1): 72.01%\n",
      "🎯 Test Accuracy (Top-5): 88.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct_top1, correct_top5, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for joint_batch, bone_batch, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "        joint_batch, bone_batch, labels = (\n",
    "            joint_batch.to(device),\n",
    "            bone_batch.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "    \n",
    "        outputs = model(joint_batch, bone_batch)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "    \n",
    "        # --- Top-1 accuracy ---\n",
    "        _, pred_top1 = probs.max(1)\n",
    "        correct_top1 += pred_top1.eq(labels).sum().item()\n",
    "    \n",
    "        # --- Top-5 accuracy ---\n",
    "        _, pred_top5 = probs.topk(5, 1, True, True)\n",
    "        correct_top5 += pred_top5.eq(labels.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "    \n",
    "        total += labels.size(0)\n",
    "    \n",
    "    # --- Compute final accuracies ---\n",
    "    top1_acc = 100.0 * correct_top1 / total\n",
    "    top5_acc = 100.0 * correct_top5 / total\n",
    "    \n",
    "    return top1_acc, top5_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================\n",
    "top1, top5 = evaluate_model(model, test_loader, DEVICE)\n",
    "print(f\"\\n🎯 Test Accuracy (Top-1): {top1:.2f}%\")\n",
    "print(f\"🎯 Test Accuracy (Top-5): {top5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on 100 Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:01:17.468352Z",
     "iopub.status.busy": "2025-11-02T10:01:17.468111Z",
     "iopub.status.idle": "2025-11-02T10:01:19.889383Z",
     "shell.execute_reply": "2025-11-02T10:01:19.888723Z",
     "shell.execute_reply.started": "2025-11-02T10:01:17.468336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration loaded. Batch size: 24, Epochs: 150\n",
      "\n",
      "Loading JSON data...\n",
      "Number of classes: 100\n",
      "Train: 1441, Val: 338, Test: 258\n",
      "\n",
      "Creating datasets with augmentation...\n",
      "Dataloaders created. Train batches: 61\n",
      "\n",
      "Initializing TT-STGCN model...\n",
      "Total trainable parameters: 1,097,405\n",
      "\n",
      "Setting up training components...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Paths\n",
    "FEATURES_DIR = '/kaggle/input/msegcn-individual/MSE_GCN_features_individual'\n",
    "JSON_PATH = '/kaggle/input/json-files/nslt_100.json'\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 24  # Increased for better gradient estimates\n",
    "EPOCHS = 150  # More epochs with better regularization\n",
    "BASE_CHANNELS = 32  # Lightweight\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.05\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MIXUP_ALPHA = 0.2\n",
    "\n",
    "print(f\"Configuration loaded. Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA AND CREATE SPLITS\n",
    "# ============================================================================\n",
    "print(\"\\nLoading JSON data...\")\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "train_ids, val_ids, test_ids = [], [], []\n",
    "labels_dict = {}\n",
    "\n",
    "unique_labels = sorted({info['action'][0] for info in json_data.values()})\n",
    "class_to_idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "for video_id, info in json_data.items():\n",
    "    file_path = os.path.join(FEATURES_DIR, f\"{video_id}.npz\")\n",
    "    if os.path.exists(file_path):\n",
    "        original_label = info['action'][0]\n",
    "        labels_dict[video_id] = class_to_idx[original_label]\n",
    "        \n",
    "        if info['subset'] == 'train':\n",
    "            train_ids.append(video_id)\n",
    "        elif info['subset'] == 'val':\n",
    "            val_ids.append(video_id)\n",
    "        elif info['subset'] == 'test':\n",
    "            test_ids.append(video_id)\n",
    "\n",
    "print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")\n",
    "\n",
    "\n",
    "print(\"\\nCreating datasets with augmentation...\")\n",
    "\n",
    "# Augmentation for training\n",
    "augmentation = SignLanguageAugmentation(\n",
    "    shear_range=0.1,\n",
    "    rotate_range=20,\n",
    "    scale_range=(0.9, 1.1),\n",
    "    temporal_crop_ratio=0.9,\n",
    "    spatial_drop_prob=0.1,\n",
    "    temporal_mask_prob=0.15\n",
    ")\n",
    "\n",
    "train_dataset = EnhancedSignLanguageDataset(\n",
    "    train_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=augmentation, training=True\n",
    ")\n",
    "\n",
    "val_dataset = EnhancedSignLanguageDataset(\n",
    "    val_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=None, training=False\n",
    ")\n",
    "\n",
    "test_dataset = EnhancedSignLanguageDataset(\n",
    "    test_ids, labels_dict, FEATURES_DIR, \n",
    "    seq_len=SEQ_LEN, augmentation=None, training=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Dataloaders created. Train batches: {len(train_loader)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INSTANTIATE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\nInitializing TT-STGCN model...\")\n",
    "model = TT_STGCN(num_classes=num_classes, base_ch=BASE_CHANNELS).to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP TRAINING COMPONENTS\n",
    "# ============================================================================\n",
    "print(\"\\nSetting up training components...\")\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# Optimizer: AdamW with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Scheduler: Cosine annealing with warmup\n",
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, warmup_epochs=10, max_epochs=EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:04:00.534519Z",
     "iopub.status.busy": "2025-11-02T10:04:00.534220Z",
     "iopub.status.idle": "2025-11-02T10:04:00.785466Z",
     "shell.execute_reply": "2025-11-02T10:04:00.784739Z",
     "shell.execute_reply.started": "2025-11-02T10:04:00.534497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded best model from: /kaggle/input/ttstgcn/other/default/1/best_tt_stgcn.pth\n",
      "✅ Loaded 258 test samples.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "BEST_MODEL_PATH = \"/kaggle/input/ttstgcn/other/default/1/best_tt_stgcn.pth\" # Change path if needed\n",
    "\n",
    "\n",
    "\n",
    "model = TT_STGCN(num_classes=300, base_ch=BASE_CHANNELS).to(DEVICE)\n",
    "checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "# If checkpoint contains model_state_dict, extract it\n",
    "if \"model_state_dict\" in checkpoint:\n",
    "    state_dict = checkpoint[\"model_state_dict\"]\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ Loaded best model from: {BEST_MODEL_PATH}\")\n",
    "\n",
    "\n",
    "test_augmentation = SignLanguageAugmentation() # no effect since training=False\n",
    "test_dataset = EnhancedSignLanguageDataset(\n",
    "test_ids,\n",
    "labels_dict,\n",
    "FEATURES_DIR,\n",
    "seq_len=SEQ_LEN,\n",
    "augmentation=test_augmentation,\n",
    "training=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"✅ Loaded {len(test_dataset)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T10:04:02.538461Z",
     "iopub.status.busy": "2025-11-02T10:04:02.538153Z",
     "iopub.status.idle": "2025-11-02T10:04:04.504602Z",
     "shell.execute_reply": "2025-11-02T10:04:04.503640Z",
     "shell.execute_reply.started": "2025-11-02T10:04:02.538428Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Test Accuracy (Top-1): 74.03%\n",
      "🎯 Test Accuracy (Top-5): 90.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct_top1, correct_top5, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for joint_batch, bone_batch, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "        joint_batch, bone_batch, labels = (\n",
    "            joint_batch.to(device),\n",
    "            bone_batch.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "    \n",
    "        outputs = model(joint_batch, bone_batch)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "    \n",
    "        # --- Top-1 accuracy ---\n",
    "        _, pred_top1 = probs.max(1)\n",
    "        correct_top1 += pred_top1.eq(labels).sum().item()\n",
    "    \n",
    "        # --- Top-5 accuracy ---\n",
    "        _, pred_top5 = probs.topk(5, 1, True, True)\n",
    "        correct_top5 += pred_top5.eq(labels.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "    \n",
    "        total += labels.size(0)\n",
    "    \n",
    "    # --- Compute final accuracies ---\n",
    "    top1_acc = 100.0 * correct_top1 / total\n",
    "    top5_acc = 100.0 * correct_top5 / total\n",
    "    \n",
    "    return top1_acc, top5_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================\n",
    "top1, top5 = evaluate_model(model, test_loader, DEVICE)\n",
    "print(f\"\\n🎯 Test Accuracy (Top-1): {top1:.2f}%\")\n",
    "print(f\"🎯 Test Accuracy (Top-5): {top5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8335217,
     "sourceId": 13155294,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8388980,
     "sourceId": 13238301,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8440669,
     "sourceId": 13314792,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 483555,
     "modelInstanceId": 467731,
     "sourceId": 621868,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 487944,
     "modelInstanceId": 472044,
     "sourceId": 626982,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
